\section{Análisis Exploratorio de Datos y Limpieza}

Se desarrollaron procesos de análisis de datos para las bases de siniestros históricos y pólizas vendidas, con posterior desagregación del modelo colectivo por tipos de cobertura.

\subsection{Procesamiento de Datos de Siniestros}

El análisis de la base de datos histórica de siniestros utilizó el archivo \texttt{Siniestros\_Hist.xlsx}, que contenía 23,363 registros con 14 columnas. El procesamiento incluyó las siguientes etapas:\\

\textbf{Reestructuración y renombrado de columnas:} Se realizó un renombrado sistemático de las 14 columnas originales del dataset para mejorar la interpretación, transformando códigos abreviados (e.g., \texttt{FECHASIN}, \texttt{VLRPRIMAPAG}, \texttt{VLRSININCUR}, \texttt{COBERTURA\_FINAL}) a nombres descriptivos en español (e.g., \texttt{Fecha\_siniestro}, \texttt{Prima\_efectivamente\_pagada\_hasta\_} \texttt{fecha\_siniestro}, \texttt{Siniestro\_incurrido}, \texttt{Cobertura\_final\_aplicada}).\\

\textbf{Categorización y recodificación de coberturas:} Se identificaron cinco tipos de cobertura originales (\textbf{PTH}, \textbf{PPD}, \textbf{RC BIENES}, \textbf{PPH}, \textbf{RC PERS}) que se recodificaron a cuatro categorías mediante transformación de factor, consolidando las coberturas de responsabilidad civil (\textbf{RC BIENES} y \textbf{RC PERS}) en una sola categoría \texttt{Responsabilidad\_civil} para el análisis actuarial.\\

\textbf{Creación de variables temporales:} Se crearon cuatro variables adicionales (\texttt{Año}, \texttt{Mes}, \texttt{Día}, \texttt{Periodo}) para facilitar el análisis temporal y el cruce con datos del IPC, utilizando \texttt{format()} para extraer componentes de fecha y calculando \texttt{Periodo} como \texttt{Año * 100 + Mes} para el merge.\\

\textbf{Filtrado temporal:} Se aplicó filtro estricto para incluir únicamente siniestros ocurridos durante el año 2018, reduciendo el dataset original a los registros correspondientes al período de análisis. El filtro resultó en un rango de fechas de 2018-01-01 a 2018-12-31.\\

\textbf{Ajuste por inflación:} Se implementó ajuste por inflación utilizando el archivo \texttt{IPC\_Update.csv} que contenía factores de corrección mensuales para 2018. El proceso incluyó:
\begin{itemize}
    \item Merge por variable \texttt{Periodo} (de siniestros) con \texttt{Key} (de IPC)
    \item Aplicación de factores \texttt{IPCS\_Update} para crear \texttt{Siniestro\_incurrido\_update}
    \item Actualización de valores monetarios al poder adquisitivo de enero 2019
\end{itemize}

\textbf{Selección de variables y procesamiento final:} Se seleccionaron únicamente las variables necesarias para el análisis: \texttt{Fecha\_siniestro}, \texttt{Año}, \texttt{Mes}, \texttt{Día}, \texttt{Periodo}, \texttt{Siniestro\_incurrido}, \texttt{Cobertura\_final\_aplicada}, \texttt{IPCS\_Update}, y \texttt{Siniestro\_incurrido\_update}.\\

\textbf{Segmentación y agregación por cobertura:} Los datos se procesaron independientemente para cada tipo de cobertura utilizando \texttt{group\_by} y \texttt{summarise}, calculando para cada día:
\begin{itemize}
    \item \textbf{Cantidad\_siniestros}: Conteo de siniestros por día usando \texttt{n()}
    \item \textbf{Valor\_siniestro\_incurrido\_update}: Suma de valores ajustados por inflación usando \texttt{sum()}
\end{itemize}

Este procesamiento generó cuatro datasets específicos:
\begin{itemize}
    \item \textbf{Siniestros\_ppd}: Pérdida Parcial por Daños (mayor frecuencia, registros diarios consistentes)
    \item \textbf{Siniestros\_pth}: Pérdida Total por Hurto (menor frecuencia, mayor severidad)
    \item \textbf{Siniestros\_pph}: Pérdida Parcial por Hurto (frecuencia intermedia)
    \item \textbf{Siniestros\_rc}: Responsabilidad Civil (patrones específicos de frecuencia y severidad)
\end{itemize}

\subsection{Procesamiento de Datos de Pólizas}

El análisis de la base de datos de pólizas utilizó el archivo \texttt{polizas\_v2.txt} en formato de texto con separador "|", implementando un proceso integral de limpieza y depuración:\\

\textbf{Carga y conversión inicial:} Los datos se cargaron desde formato .txt con separador "|" usando \texttt{read.table()} y se convirtieron a CSV para facilitar el manejo posterior. El dataset original contenía 10 columnas.\\

\textbf{Reestructuración y renombrado de columnas:} Se realizó renombrado sistemático de las 10 columnas originales, transformando códigos abreviados (\texttt{FECINICIO}, \texttt{FECFIN}, \texttt{VLRPRISUSCR}, indicadores de cobertura) a nombres descriptivos (\texttt{Fecha\_inicio}, \texttt{Fecha\_fin}, \texttt{Prima}, \texttt{Perdida\_parcial\_danos}, etc.).\\

\textbf{Eliminación de columnas redundantes:} Se eliminaron tres columnas para simplificar el análisis y evitar duplicación: \texttt{Perdida\_total\_dano}, \texttt{Valor\_asegurado}, y \texttt{Valor\_asegurado\_rc}.\\

\textbf{Tratamiento de duplicados y valores faltantes:} Se aplicó \texttt{unique()} para eliminar registros duplicados (cantidad no documentada específicamente). El análisis de valores faltantes reveló 174 registros con valores nulos (0.004\% del total de datos), concentrados exclusivamente en la variable \texttt{Prima}. Estos registros se eliminaron utilizando \texttt{na.omit()}.\\

\textbf{Conversión de tipos de datos:} Se convirtieron las columnas de fecha utilizando \texttt{as.Date()} para \texttt{Fecha\_inicio} y \texttt{Fecha\_fin}, facilitando el análisis temporal posterior.\\

\textbf{Filtrado temporal:} Se aplicó filtro para incluir únicamente pólizas con vigencia durante 2018, utilizando el criterio: \texttt{Fecha\_inicio <= "2018-12-31" | Fecha\_fin >= "2018-01-01"}. Este criterio aseguró la captura de todas las pólizas que tuvieron exposición durante el año de análisis.\\

\textbf{Análisis y filtrado por valor de prima:} Se implementó un análisis detallado del valor de prima que incluyó:
\begin{itemize}
    \item Establecimiento de umbral mínimo de \$459,500 pesos basado en tarifas SOAT 2016 para vehículos con cilindraje menor a 1500cc
    \item Análisis cuantitativo que mostró que las pólizas con prima inferior al umbral representaban el 14.08\% del total de pólizas pero únicamente el 2.04\% del valor total de primas
    \item Aplicación del filtro \texttt{Prima >= 459500} que conservó 382,530 pólizas para el análisis final
\end{itemize}

\textbf{Análisis de duración de pólizas:} Se calculó la duración como \texttt{Fecha\_fin - Fecha\_inicio} y se realizó análisis de anomalías:
\begin{itemize}
    \item Identificación de 1,272 pólizas con duración de 0 días (0.33\% del total)
    \item Estas pólizas representaban el 0.70\% del valor total de primas
    \item Identificación de 1,659 pólizas con duración inferior a 30 días
    \item Decisión de mantener estas pólizas tras confirmar que no representaban anomalías significativas
\end{itemize}

\textbf{Ajuste por inflación:} Se implementó ajuste por inflación utilizando la función \texttt{ajuste\_prima\_ipc()} que:
\begin{itemize}
    \item Aplicó factores de corrección del IPC histórico específicos para cada período
    \item Creó la variable \texttt{Prima\_ajustada} expresando todas las primas en poder adquisitivo constante
    \item Facilitó la comparabilidad temporal de los datos monetarios
\end{itemize}

\textbf{Cálculo de exposición diaria por cobertura:} Se procesaron los datos utilizando la función \texttt{polizas\_diarias()} para generar métricas diarias durante 2018. Para cada día del año y cada tipo de cobertura se calculó:
\begin{itemize}
    \item \textbf{Numero\_polizas}: Conteo de pólizas vigentes con la cobertura específica
    \item \textbf{Suma\_primas}: Suma total de primas ajustadas por inflación
    \item \textbf{Exposicion}: Suma de días de exposición restantes dividida por 365 (expresada en años)
\end{itemize}

\textbf{Generación de datasets finales:} El procesamiento resultó en cuatro datasets específicos correspondientes a cada tipo de cobertura:
\begin{itemize}
    \item \textbf{polizas\_ppd.csv}: Pérdida Parcial por Daños
    \item \textbf{polizas\_pth.csv}: Pérdida Total por Hurto (renombrado de PH)
    \item \textbf{polizas\_pph.csv}: Pérdida Parcial por Hurto
    \item \textbf{polizas\_rc.csv}: Responsabilidad Civil
\end{itemize}

Cada dataset contiene 365 registros (uno por día del 2018) con las métricas de exposición correspondientes, constituyendo la base para el cálculo de tasas de frecuencia y la modelación actuarial posterior. La metodología aseguró la consistencia temporal entre las bases de pólizas y siniestros, y la calidad de los datos mediante filtros apropiados basados en estándares regulatorios del sector asegurador colombiano.